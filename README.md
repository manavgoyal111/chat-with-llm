# ğŸ§  Ollama Multimodal Chat Interface

A modern multimodal AI chat UI that supports **text**, **voice**, and **image inputs**, backed by **Ollama** for running LLMs locally. Built using **React**, **Gradio**, **FastAPI (optional)**, and integrated with multiple LLMs (DeepSeek, Mistral, LLaMA3, etc.).

---

## âœ¨ Features

- ğŸ”¤ **Text input**: Ask questions or give instructions in plain text.
- ğŸ¤ **Voice input**: Upload `.wav` audio or speak directly (speech-to-text using Google Speech API).
- ğŸ–¼ï¸ **Image input**: Upload image files for OCR using `easyocr`.
- ğŸ¤– **Model selection**: Choose from available Ollama models dynamically.
- ğŸ—ƒï¸ **Conversation history**: Stored per-session with `conversation_id`.
- ğŸ’¾ **Export chat**: Download chat as JSON.
- ğŸš€ **Streaming replies**: Simulated (can be expanded with websocket).
- ğŸ§ª **Model introspection**: Auto-fetch from `/api/tags` and fall back to defaults.

---

## ğŸ—ï¸ Tech Stack

| Layer             | Tool / Library                                      |
|------------------|------------------------------------------------------|
| ğŸ’» Frontend       | React + TypeScript + TailwindCSS                    |
| ğŸ§  Backend        | Python, Gradio, optionally FastAPI or Flask         |
| ğŸ§± UI Components  | shadcn/ui, Lucide Icons                             |
| ğŸ“¡ API Calls      | Gradio client, REST fetch, dynamic model resolver  |
| ğŸ” OCR            | `easyocr` (Python)                                  |
| ğŸ™ï¸ Voice-to-text  | `speech_recognition` using Google API               |
| ğŸ”— LLMs           | Ollama (LLaMA3, Mistral, DeepSeek, Qwen, etc.)     |

---

## ğŸ”§ Setup Instructions

1. **Install dependencies**:
   ```bash
      python -m venv venv
      venv\Scripts\activate.bat
      pip install gradio easyocr speechrecognition ollama
      npm i
   ```
2. **Run App**:
   ```bash
      npm start
   ```
